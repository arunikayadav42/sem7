{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train dataset for XOR , truth table of XOR\n",
    "train_X = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output for XOR, based on index, if index 0 is 1 : i.e output is 0, if index 1 is 1 : otput is 1\n",
    "train_Y = np.array([[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]])\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the test dataset\n",
    "test_X = np.array([[1.0, 0.0], [0.0 , 0.0]])\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output for test\n",
    "test_Y = np.array([[0.0, 1.0], [1.0, 0.0]])\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defininf the MLP\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    self.layer_numberofnodes=list()\n",
    "    self.W=list()\n",
    "#     self.inputSize = int(input('No .of nodes in input layer :'))\n",
    "#     self.outputSize = int(input('No. of nodes in output layer :'))\n",
    "#     self.nhiddenlayers= int((input('No of hidden layers :')))\n",
    "    self.inputSize = 2\n",
    "    self.layer_numberofnodes.append(self.inputSize)\n",
    "    self.outputSize = 2\n",
    "    self.nhiddenlayers = 1\n",
    "\n",
    "#     for i in range(self.nhiddenlayers):\n",
    "#       print(\"No. of nodes in hidden layer {} : \".format(i+1))\n",
    "#       self.layer_numberofnodes.append(int(input()))\n",
    "    self.layer_numberofnodes.append(2)\n",
    "    self.layer_numberofnodes.append(self.outputSize)\n",
    "\n",
    "    self.layerlist=[None]*(self.nhiddenlayers + 2)\n",
    "\n",
    "    #weights\n",
    "    for i in range(len(self.layer_numberofnodes)-1):\n",
    "      self.W.append(np.random.randn(self.layer_numberofnodes[i],self.layer_numberofnodes[i+1]))\n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "    self.layerlist[0]=X\n",
    "    self.z =  np.dot(X, self.W[0]) \n",
    "    for i in range(1,(len(self.W))):\n",
    "      #print('length layer list iteration {} = {} start'.format(i,len(self.layerlist)))\n",
    "      self.z = self.sigmoid(self.z)\n",
    "      self.layerlist[i]=self.z\n",
    "      self.z = np.dot(self.z,self.W[i])\n",
    "      #print('length layer list iteration {} = {} end'.format(i,len(self.layerlist)))\n",
    "    o=self.sigmoid(self.z)\n",
    "    self.layerlist[self.nhiddenlayers+1]=o\n",
    "    return o \n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function \n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def Relu(self, s):\n",
    "    return np.maximum(0,s)\n",
    "\n",
    "  def ReluPrime(self,s):\n",
    "    if(s==0): \n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "  def backward(self, X, y, o,lr):\n",
    "\n",
    "    self.z_error = y - o\n",
    "    for i in range(len(self.layerlist)-1,0,-1):\n",
    "      self.z_delta = self.z_error*self.sigmoidPrime(self.layerlist[i])\n",
    "      self.z_error = self.z_delta.dot(self.W[i-1].T ) \n",
    "      self.W[i-1] += lr*self.layerlist[i-1].T.dot(self.z_delta)\n",
    "\n",
    "  def train (self, X, y,lr):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining test_loss_values for storing test_loss values against multiple learning rates\n",
    "test_loss_values = []\n",
    "learning_rate_values = [float(i) for i in [0.5, 0.1, 0.05, 0.01, 0.001, 0.0001, 0.00001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  1 \n",
      "Learning rate :  0.5\n",
      "Training Loss: \n",
      "0.2987439699304221\n",
      "Training Loss: \n",
      "0.29281608012178434\n",
      "Training Loss: \n",
      "0.28752307567963353\n",
      "Training Loss: \n",
      "0.28291350287204875\n",
      "Training Loss: \n",
      "0.27898046187217607\n",
      "Test Loss: \n",
      "0.2405944086622657\n",
      "\n",
      "Iteration  2 \n",
      "Learning rate :  0.1\n",
      "Training Loss: \n",
      "0.25447675973760686\n",
      "Training Loss: \n",
      "0.2543630698260575\n",
      "Training Loss: \n",
      "0.2542518920291589\n",
      "Training Loss: \n",
      "0.2541431672853944\n",
      "Training Loss: \n",
      "0.254036837989256\n",
      "Test Loss: \n",
      "0.27898772945665484\n",
      "\n",
      "Iteration  3 \n",
      "Learning rate :  0.05\n",
      "Training Loss: \n",
      "0.2557277135128583\n",
      "Training Loss: \n",
      "0.25563420359373823\n",
      "Training Loss: \n",
      "0.2555419613222878\n",
      "Training Loss: \n",
      "0.2554509729285349\n",
      "Training Loss: \n",
      "0.25536122470133515\n",
      "Test Loss: \n",
      "0.27020917528576394\n",
      "\n",
      "Iteration  4 \n",
      "Learning rate :  0.01\n",
      "Training Loss: \n",
      "0.25130559331640834\n",
      "Training Loss: \n",
      "0.25130156370771195\n",
      "Training Loss: \n",
      "0.2512975472148235\n",
      "Training Loss: \n",
      "0.25129354379544655\n",
      "Training Loss: \n",
      "0.2512895534074172\n",
      "Test Loss: \n",
      "0.25077808555926573\n",
      "\n",
      "Iteration  5 \n",
      "Learning rate :  0.001\n",
      "Training Loss: \n",
      "0.2548553498602013\n",
      "Training Loss: \n",
      "0.2548538379956088\n",
      "Training Loss: \n",
      "0.25485232663978064\n",
      "Training Loss: \n",
      "0.2548508157925539\n",
      "Training Loss: \n",
      "0.2548493054537656\n",
      "Test Loss: \n",
      "0.25937128698852413\n",
      "\n",
      "Iteration  6 \n",
      "Learning rate :  0.0001\n",
      "Training Loss: \n",
      "0.2882839336548324\n",
      "Training Loss: \n",
      "0.28828324238303465\n",
      "Training Loss: \n",
      "0.2882825511193287\n",
      "Training Loss: \n",
      "0.28828185986371463\n",
      "Training Loss: \n",
      "0.28828116861619246\n",
      "Test Loss: \n",
      "0.28745882670222\n",
      "\n",
      "Iteration  7 \n",
      "Learning rate :  1e-05\n",
      "Training Loss: \n",
      "0.2522259912097624\n",
      "Training Loss: \n",
      "0.2522259844295557\n",
      "Training Loss: \n",
      "0.25222597764937266\n",
      "Training Loss: \n",
      "0.25222597086921345\n",
      "Training Loss: \n",
      "0.25222596408907805\n",
      "Test Loss: \n",
      "0.2488266968475826\n"
     ]
    }
   ],
   "source": [
    "#computing loss values against different learning rates\n",
    "for i in range(len(learning_rate_values)):\n",
    "    print(\"\\nIteration \", i+1, \"\\nLearning rate : \", learning_rate_values[i])\n",
    "    NN = Neural_Network()\n",
    "    epoch = 5\n",
    "    learning_rate = learning_rate_values[i]\n",
    "    for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "      #print (\"Input: \\n\" + str(X))\n",
    "    #   print (\"Actual Output: \\n\" + str(y))\n",
    "      print (\"Training Loss: \\n\" + str((np.mean(np.square(train_Y - NN.forward(train_X))))))# mean sum squared loss\n",
    "      NN.train(train_X, train_Y, learning_rate)\n",
    "    test_loss_values.append((np.mean(np.square(test_Y - NN.forward(test_X)))))\n",
    "    print (\"Test Loss: \\n\" + str(test_loss_values[i]))# mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test loss')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting loss_value v/s learning rate\n",
    "test_loss_values\n",
    "plt.plot(learning_rate_values, test_loss_values, 'ro')\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: \n",
      "0.2646579571178097\n",
      "Training Loss: \n",
      "0.26361980494351533\n",
      "Training Loss: \n",
      "0.2626413276563744\n",
      "Training Loss: \n",
      "0.26172214716129294\n",
      "Training Loss: \n",
      "0.2608613800653824\n",
      "Test Loss: \n",
      "0.21513881727850245\n"
     ]
    }
   ],
   "source": [
    "#for the best observed learning rate, using the neural network to predict the test outputs \n",
    "NN = Neural_Network()  \n",
    "learning_rate = 0.5\n",
    "NN.train(train_X, train_Y, learning_rate)\n",
    "epoch = 5\n",
    "for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "    print (\"Training Loss: \\n\" + str((np.mean(np.square(train_Y - NN.forward(train_X))))))# mean sum squared loss\n",
    "    NN.train(train_X, train_Y, learning_rate)\n",
    "\n",
    "print (\"Test Loss: \\n\" + str(np.mean(np.square(test_Y - NN.forward(test_X)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed forward prediction on the test dataset\n",
    "output = NN.forward(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predictions for the test dataset\n",
    "y_predicted = []\n",
    "for row in output : \n",
    "    y_predicted.append(np.argmax(row))\n",
    "y_predicted = np.array(y_predicted)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#considering actual test output for predicted dataset\n",
    "y_test_output = []\n",
    "for row in test_Y : \n",
    "    y_test_output.append(np.argmax(row))\n",
    "y_test_output = np.array(y_test_output)\n",
    "y_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#computing the confusion matrix for the dataset\n",
    "confusion_matrix = np.zeros([2,2])\n",
    "for i in range(2):\n",
    "    confusion_matrix[y_test_output[i]][y_predicted[i]] += 1\n",
    "        \n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting true positive using confusion matrix\n",
    "TruePositive = np.diag(confusion_matrix) \n",
    "TruePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting false positive using confusion matrix\n",
    "FalsePositive = []\n",
    "for i in range(2): \n",
    "    FalsePositive.append(sum(confusion_matrix[:,i]) - confusion_matrix[i,i])\n",
    "FalsePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting False Negative using confusion matrix\n",
    "FalseNegative = []\n",
    "for i in range(2): \n",
    "    FalseNegative.append(sum(confusion_matrix[i, :]) - confusion_matrix[i,i])\n",
    "FalseNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting true negative using confusion matrix\n",
    "TrueNegative = []\n",
    "for i in range(2): \n",
    "    temp = np.delete(confusion_matrix, i, 0) # delete ith row \n",
    "    temp = np.delete(temp, i, 1) # delete ith column \n",
    "    TrueNegative.append(sum(sum(temp))) \n",
    "TrueNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class  1\n",
      "Accuracy :  0.5\n",
      "Precision :  0.5\n",
      "Recall :  1.0\n",
      "F-Score :  0.6666666666666666\n",
      "\n",
      "Class  2\n",
      "Accuracy :  0.5\n",
      "Precision :  nan\n",
      "Recall :  0.0\n",
      "F-Score :  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#computing classwise accuracy precision recall and F-Score\n",
    "for i in range(2):\n",
    "    print(\"\\nClass \", i+1)\n",
    "    print(\"Accuracy : \", (TruePositive[i] + TrueNegative[i]) / (TruePositive[i] + TrueNegative[i] + FalsePositive[i] + FalseNegative[i]))\n",
    "    precision = (TruePositive[i]) / (TruePositive[i] + FalsePositive[i])\n",
    "    print(\"Precision : \", precision)\n",
    "    recall = (TruePositive[i]) / (TruePositive[i] + FalseNegative[i])\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"F-Score : \", (2 * precision * recall) / (precision + recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
