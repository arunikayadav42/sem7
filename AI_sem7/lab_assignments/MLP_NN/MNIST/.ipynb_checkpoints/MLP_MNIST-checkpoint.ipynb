{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mayankw/AI Assignment/MNIST\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert the image file dataset to flattened dataset with pixel values between 0 and 255\n",
    "#by removing the first 16 bits ( header infromation) the 8 more bits (label header information)\n",
    "#briefly explained in line-specific comments \n",
    "\n",
    "def convert(img_bin_file, lbl_bin_file,\n",
    "           img_txt_file, lbl_txt_file, n_images):\n",
    "\n",
    "   img_bf = open(img_bin_file, \"rb\")    # binary image pixels\n",
    "   lbl_bf = open(lbl_bin_file, \"rb\")    # binary labels\n",
    "\n",
    "   img_tf = open(img_txt_file, \"w\")     # text image pixels\n",
    "   lbl_tf = open(lbl_txt_file, \"w\")     # text labels\n",
    "\n",
    "   img_bf.read(16)   # discard image header info\n",
    "   lbl_bf.read(8)    # discard label header info\n",
    "\n",
    "   for i in range(n_images):   # number images requested\n",
    "\n",
    "       # do labels first for no particular reason\n",
    "       lbl = ord(lbl_bf.read(1))  # get label like '3' (one byte)\n",
    "       encoded = [0] * 10         # make one-hot vector\n",
    "       encoded[lbl] = 1\n",
    "       for i in range(10):\n",
    "           lbl_tf.write(str(encoded[i]))\n",
    "           if i != 9:\n",
    "               lbl_tf.write(\" \")  # like 0 0 0 1 0 0 0 0 0 0\n",
    "       lbl_tf.write(\"\\n\")\n",
    "\n",
    "       # now do the image pixels\n",
    "       for j in range(784):  # get 784 vals for each image file\n",
    "           val = ord(img_bf.read(1))\n",
    "           img_tf.write(str(val))\n",
    "           if j != 783:\n",
    "               img_tf.write(\" \")  # avoid trailing space\n",
    "       img_tf.write(\"\\n\")  # next image\n",
    "\n",
    "   img_bf.close()\n",
    "   lbl_bf.close()  # close the binary files\n",
    "   img_tf.close()\n",
    "   lbl_tf.close()   # close the text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the train images and corresponding train labels to 784 (28 * 28) to flattened vector dataset\n",
    "convert(\"train-images.idx3-ubyte\",\n",
    "       \"train-labels.idx1-ubyte\",\n",
    "       \"mnist_train_images.txt\",\n",
    "       \"mnist_train_labels.txt\",60000)  # first n images\n",
    "\n",
    "#converting the test images and corresponding test labels to 784 (28 * 28) to flattened vector dataset\n",
    "convert(\"t10k-images.idx3-ubyte\",\n",
    "       \"t10k-labels.idx1-ubyte\",\n",
    "       \"mnist_test_images.txt\",\n",
    "       \"mnist_test_labels.txt\",10000)  # first n images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the train dataset and storing as numpy array, then normalizing it\n",
    "x_train = pd.read_csv(\"mnist_train_images.txt\", sep=' ', lineterminator ='\\n', header = None).to_numpy()\n",
    "x_train=(x_train-x_train.min())/(x_train.max() - x_train.min())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing the outputs ( actual ) for the training dataset in another numpy array\n",
    "y_train = pd.read_csv(\"mnist_train_labels.txt\", sep=' ', lineterminator ='\\n', header = None).to_numpy()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the test dataset and storing as numpy array, then normalizing it\n",
    "x_test = pd.read_csv(\"mnist_test_images.txt\", sep=' ', lineterminator ='\\n', header = None).to_numpy()\n",
    "x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#storing the outputs ( actual ) for the training dataset in another numpy array\n",
    "y_test = pd.read_csv(\"mnist_test_labels.txt\", sep=' ', lineterminator ='\\n', header = None).to_numpy()\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the neural network ( MLP) with 784 input nodes, 2 hidden layers with 200, 50 nodes, output layer with 10 nodes\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    self.layer_numberofnodes=list()\n",
    "    self.W=list()\n",
    "#     self.inputSize = int(input('No .of nodes in input layer :'))\n",
    "#     self.outputSize = int(input('No. of nodes in output layer :'))\n",
    "#     self.nhiddenlayers= int((input('No of hidden layers :')))\n",
    "    self.inputSize = 784  #input layer nodes\n",
    "    self.layer_numberofnodes.append(self.inputSize)\n",
    "    self.outputSize = 10 #output layer nodes\n",
    "    self.nhiddenlayers = 2 #number of hidden layers\n",
    "\n",
    "#     for i in range(self.nhiddenlayers):\n",
    "#       print(\"No. of nodes in hidden layer {} : \".format(i+1))\n",
    "#       self.layer_numberofnodes.append(int(input()))\n",
    "    self.layer_numberofnodes.append(200) #nodes in first hidden layer\n",
    "    self.layer_numberofnodes.append(50)  #nodes in second hidden layer\n",
    "    self.layer_numberofnodes.append(self.outputSize)\n",
    "\n",
    "    self.layerlist=[None]*(self.nhiddenlayers + 2)\n",
    "\n",
    "    #weights\n",
    "    for i in range(len(self.layer_numberofnodes)-1):\n",
    "      self.W.append(np.random.randn(self.layer_numberofnodes[i],self.layer_numberofnodes[i+1]))\n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "    self.layerlist[0]=X\n",
    "    self.z =  np.dot(X, self.W[0]) \n",
    "    for i in range(1,(len(self.W))):\n",
    "      #print('length layer list iteration {} = {} start'.format(i,len(self.layerlist)))\n",
    "      self.z = self.sigmoid(self.z)\n",
    "      self.layerlist[i]=self.z\n",
    "      self.z = np.dot(self.z,self.W[i])\n",
    "      #print('length layer list iteration {} = {} end'.format(i,len(self.layerlist)))\n",
    "    o=self.sigmoid(self.z)\n",
    "    self.layerlist[self.nhiddenlayers+1]=o\n",
    "    return o \n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function \n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def Relu(self, s):\n",
    "    return np.maximum(0,s)\n",
    "\n",
    "  def ReluPrime(self,s):\n",
    "    if(s==0): \n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "  def backward(self, X, y, o,lr):\n",
    "\n",
    "    self.z_error = y - o\n",
    "    for i in range(len(self.layerlist)-1,0,-1):\n",
    "      self.z_delta = self.z_error*self.sigmoidPrime(self.layerlist[i])\n",
    "      self.z_error = self.z_delta.dot(self.W[i-1].T ) \n",
    "      self.W[i-1] += lr*self.layerlist[i-1].T.dot(self.z_delta)\n",
    "\n",
    "  def train (self, X, y,lr):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining test_loss_values for storing test_loss values against multiple learning rates\n",
    "test_loss_values = []\n",
    "learning_rate_values = [float(i) for i in [0.5, 0.1, 0.05, 0.01, 0.001, 0.0001, 0.00001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  1 \n",
      "Learning rate :  0.5\n",
      "Training Loss: \n",
      "0.4784519615810314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Test Loss: \n",
      "0.1\n",
      "\n",
      "Iteration  2 \n",
      "Learning rate :  0.1\n",
      "Training Loss: \n",
      "0.2966800426796847\n",
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Training Loss: \n",
      "0.1\n",
      "Test Loss: \n",
      "0.1\n",
      "\n",
      "Iteration  3 \n",
      "Learning rate :  0.05\n",
      "Training Loss: \n",
      "0.3647347648951204\n",
      "Training Loss: \n",
      "0.22625218017456733\n",
      "Training Loss: \n",
      "0.09999735683482554\n",
      "Training Loss: \n",
      "0.09999368721914707\n",
      "Training Loss: \n",
      "0.09995010929596185\n",
      "Test Loss: \n",
      "0.17943340182351938\n",
      "\n",
      "Iteration  4 \n",
      "Learning rate :  0.01\n",
      "Training Loss: \n",
      "0.3331775537036041\n",
      "Training Loss: \n",
      "0.09913945433399383\n",
      "Training Loss: \n",
      "0.3273008337186817\n",
      "Training Loss: \n",
      "0.2598366206636451\n",
      "Training Loss: \n",
      "0.2598366205238306\n",
      "Test Loss: \n",
      "0.2606399536552121\n",
      "\n",
      "Iteration  5 \n",
      "Learning rate :  0.001\n",
      "Training Loss: \n",
      "0.47790800428435914\n",
      "Training Loss: \n",
      "0.1782108981529598\n",
      "Training Loss: \n",
      "0.09964090453852895\n",
      "Training Loss: \n",
      "0.09935231795572565\n",
      "Training Loss: \n",
      "0.09872429389016156\n",
      "Test Loss: \n",
      "0.09823160529861265\n",
      "\n",
      "Iteration  6 \n",
      "Learning rate :  0.0001\n",
      "Training Loss: \n",
      "0.41309311431565365\n",
      "Training Loss: \n",
      "0.16590346381970084\n",
      "Training Loss: \n",
      "0.10002146883932793\n",
      "Training Loss: \n",
      "0.09971797407993953\n",
      "Training Loss: \n",
      "0.09957949227745816\n",
      "Test Loss: \n",
      "0.09950918726325939\n",
      "\n",
      "Iteration  7 \n",
      "Learning rate :  1e-05\n",
      "Training Loss: \n",
      "0.23317793652214003\n",
      "Training Loss: \n",
      "0.20094728915803103\n",
      "Training Loss: \n",
      "0.1715029001208318\n",
      "Training Loss: \n",
      "0.14661908808769264\n",
      "Training Loss: \n",
      "0.1295451641350952\n",
      "Test Loss: \n",
      "0.12026535292987432\n"
     ]
    }
   ],
   "source": [
    "#computing loss values against different learning rates\n",
    "for i in range(len(learning_rate_values)):\n",
    "    print(\"\\nIteration \", i+1, \"\\nLearning rate : \", learning_rate_values[i])\n",
    "    NN = Neural_Network()\n",
    "    epoch = 5\n",
    "    learning_rate = learning_rate_values[i]\n",
    "    for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "      #print (\"Input: \\n\" + str(X))\n",
    "    #   print (\"Actual Output: \\n\" + str(y))\n",
    "      print (\"Training Loss: \\n\" + str((np.mean(np.square(y_train - NN.forward(x_train))))))# mean sum squared loss\n",
    "      NN.train(x_train, y_train, learning_rate)\n",
    "    test_loss_values.append((np.mean(np.square(y_test - NN.forward(x_test)))))\n",
    "    print (\"Test Loss: \\n\" + str(test_loss_values[i]))# mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_loss_values).shape)\n",
    "print(np.array(learning_rate_values).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting loss_value v/s learning rate\n",
    "test_loss_values\n",
    "plt.plot(learning_rate_values, test_loss_values, 'ro')\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: \n",
      "0.23382779608457993\n",
      "Training Loss: \n",
      "0.15041088983755982\n",
      "Training Loss: \n",
      "0.10012070002287236\n",
      "Test Loss: \n",
      "0.0997556770316053\n"
     ]
    }
   ],
   "source": [
    "#for the best observed learning rate, using the neural network to predict the test outputs \n",
    "NN = Neural_Network()  \n",
    "learning_rate = 0.0001\n",
    "NN.train(x_train, y_train, learning_rate)\n",
    "epoch = 3\n",
    "for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "    print (\"Training Loss: \\n\" + str((np.mean(np.square(y_train - NN.forward(x_train))))))# mean sum squared loss\n",
    "    NN.train(x_train, y_train, learning_rate)\n",
    "\n",
    "print (\"Test Loss: \\n\" + str(np.mean(np.square(y_test - NN.forward(x_test)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed forward prediction on the test dataset\n",
    "output = NN.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = []\n",
    "for row in output : \n",
    "    y_predicted.append(np.argmax(row) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, ..., 2, 6, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating predictions for the test dataset\n",
    "y_predicted = np.array(y_predicted)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 3, 2, ..., 5, 6, 7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#considering actual test output for predicted dataset\n",
    "y_test_output = []\n",
    "for row in y_test : \n",
    "    y_test_output.append(np.argmax(row) + 1)\n",
    "y_test_output = np.array(y_test_output)\n",
    "y_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_output.shape)\n",
    "print(y_predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.  66. 484. ...   0.   0.   0.]\n",
      " [  0.  75. 180. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "#computing the confusion matrix for the dataset\n",
    "confusion_matrix = np.zeros(shape = [10000,10000])\n",
    "for i in range(10000):\n",
    "    confusion_matrix[y_test_output[i]][y_predicted[i]] += 1\n",
    "        \n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  66., 180., ...,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting true positive using confusion matrix\n",
    "TruePositive = np.diag(confusion_matrix) \n",
    "TruePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1063.0, 3463.0, 320.0, 1358.0, 1394.0, 644.0, 248.0, 512.0, 159.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting false positive using confusion matrix\n",
    "FalsePositive = []\n",
    "for i in range(10): \n",
    "    FalsePositive.append(sum(confusion_matrix[:,i]) - confusion_matrix[i,i])\n",
    "FalsePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 914.0, 955.0, 990.0, 842.0, 763.0, 834.0, 908.0, 981.0, 965.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting False Negative using confusion matrix\n",
    "FalseNegative = []\n",
    "for i in range(10): \n",
    "    FalseNegative.append(sum(confusion_matrix[i, :]) - confusion_matrix[i,i])\n",
    "FalseNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000.0,\n",
       " 7957.0,\n",
       " 5402.0,\n",
       " 8648.0,\n",
       " 7632.0,\n",
       " 7624.0,\n",
       " 8464.0,\n",
       " 8794.0,\n",
       " 8460.0,\n",
       " 8867.0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting true negative using confusion matrix\n",
    "TrueNegative = []\n",
    "for i in range(10): \n",
    "    temp = np.delete(confusion_matrix, i, 0) # delete ith row \n",
    "    temp = np.delete(temp, i, 1) # delete ith column \n",
    "    TrueNegative.append(sum(sum(temp))) \n",
    "TrueNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class  1\n",
      "Accuracy :  1.0\n",
      "Precision :  nan\n",
      "Recall :  nan\n",
      "F-Score :  nan\n",
      "\n",
      "Class  2\n",
      "Accuracy :  0.8023\n",
      "Precision :  0.05845881310894597\n",
      "Recall :  0.0673469387755102\n",
      "F-Score :  0.06258890469416785\n",
      "\n",
      "Class  3\n",
      "Accuracy :  0.5582\n",
      "Precision :  0.04940982706560527\n",
      "Recall :  0.15859030837004406\n",
      "F-Score :  0.07534533277521975\n",
      "\n",
      "Class  4\n",
      "Accuracy :  0.869\n",
      "Precision :  0.11602209944751381\n",
      "Recall :  0.040697674418604654\n",
      "F-Score :  0.06025824964131995\n",
      "\n",
      "Class  5\n",
      "Accuracy :  0.78\n",
      "Precision :  0.11009174311926606\n",
      "Recall :  0.16633663366336635\n",
      "F-Score :  0.13249211356466875\n",
      "\n",
      "Class  6\n",
      "Accuracy :  0.7843\n",
      "Precision :  0.13577185368877867\n",
      "Recall :  0.2230142566191446\n",
      "F-Score :  0.16878612716763003\n",
      "\n",
      "Class  7\n",
      "Accuracy :  0.8522\n",
      "Precision :  0.08262108262108261\n",
      "Recall :  0.06502242152466367\n",
      "F-Score :  0.0727728983688833\n",
      "\n",
      "Class  8\n",
      "Accuracy :  0.8844\n",
      "Precision :  0.16778523489932887\n",
      "Recall :  0.05219206680584551\n",
      "F-Score :  0.07961783439490447\n",
      "\n",
      "Class  9\n",
      "Accuracy :  0.8507\n",
      "Precision :  0.08407871198568873\n",
      "Recall :  0.04571984435797665\n",
      "F-Score :  0.05923125393824826\n",
      "\n",
      "Class  10\n",
      "Accuracy :  0.8876\n",
      "Precision :  0.05357142857142857\n",
      "Recall :  0.009240246406570842\n",
      "F-Score :  0.01576182136602452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#computing classwise accuracy precision recall and F-Score\n",
    "for i in range(10):\n",
    "    print(\"\\nClass \", i+1)\n",
    "    print(\"Accuracy : \", (TruePositive[i] + TrueNegative[i]) / (TruePositive[i] + TrueNegative[i] + FalsePositive[i] + FalseNegative[i]))\n",
    "    precision = (TruePositive[i]) / (TruePositive[i] + FalsePositive[i])\n",
    "    print(\"Precision : \", precision)\n",
    "    recall = (TruePositive[i]) / (TruePositive[i] + FalseNegative[i])\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"F-Score : \", (2 * precision * recall) / (precision + recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
