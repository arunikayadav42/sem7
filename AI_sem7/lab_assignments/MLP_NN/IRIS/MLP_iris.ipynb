{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  feature3  feature4  class\n",
       "0         5.1       3.5       1.4       0.2      0\n",
       "1         4.9       3.0       1.4       0.2      0\n",
       "2         4.7       3.2       1.3       0.2      0\n",
       "3         4.6       3.1       1.5       0.2      0\n",
       "4         5.0       3.6       1.4       0.2      0\n",
       "..        ...       ...       ...       ...    ...\n",
       "145       6.7       3.0       5.2       2.3      2\n",
       "146       6.3       2.5       5.0       1.9      2\n",
       "147       6.5       3.0       5.2       2.0      2\n",
       "148       6.2       3.4       5.4       2.3      2\n",
       "149       5.9       3.0       5.1       1.8      2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the iris dataset and rename the classes to 0, 1 and 2\n",
    "df = pd.read_csv(\"iris.data\", header = None)\n",
    "df[4] = df[4].map({\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2})\n",
    "df.columns = ['feature1', 'feature2', 'feature3', 'feature4', 'class']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.028611</td>\n",
       "      <td>-1.336794</td>\n",
       "      <td>-1.308593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.139200</td>\n",
       "      <td>-0.124540</td>\n",
       "      <td>-1.336794</td>\n",
       "      <td>-1.308593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.380727</td>\n",
       "      <td>0.336720</td>\n",
       "      <td>-1.393470</td>\n",
       "      <td>-1.308593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.501490</td>\n",
       "      <td>0.106090</td>\n",
       "      <td>-1.280118</td>\n",
       "      <td>-1.308593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.018437</td>\n",
       "      <td>1.259242</td>\n",
       "      <td>-1.336794</td>\n",
       "      <td>-1.308593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.034539</td>\n",
       "      <td>-0.124540</td>\n",
       "      <td>0.816888</td>\n",
       "      <td>1.443121</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.551486</td>\n",
       "      <td>-1.277692</td>\n",
       "      <td>0.703536</td>\n",
       "      <td>0.918985</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.793012</td>\n",
       "      <td>-0.124540</td>\n",
       "      <td>0.816888</td>\n",
       "      <td>1.050019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.430722</td>\n",
       "      <td>0.797981</td>\n",
       "      <td>0.930239</td>\n",
       "      <td>1.443121</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.068433</td>\n",
       "      <td>-0.124540</td>\n",
       "      <td>0.760212</td>\n",
       "      <td>0.787951</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  feature3  feature4  class\n",
       "0   -0.897674  1.028611 -1.336794 -1.308593      0\n",
       "1   -1.139200 -0.124540 -1.336794 -1.308593      0\n",
       "2   -1.380727  0.336720 -1.393470 -1.308593      0\n",
       "3   -1.501490  0.106090 -1.280118 -1.308593      0\n",
       "4   -1.018437  1.259242 -1.336794 -1.308593      0\n",
       "..        ...       ...       ...       ...    ...\n",
       "145  1.034539 -0.124540  0.816888  1.443121      2\n",
       "146  0.551486 -1.277692  0.703536  0.918985      2\n",
       "147  0.793012 -0.124540  0.816888  1.050019      2\n",
       "148  0.430722  0.797981  0.930239  1.443121      2\n",
       "149  0.068433 -0.124540  0.760212  0.787951      2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize the dataset\n",
    "normalized_df=(df-df.mean())/df.std()\n",
    "normalized_df['class'] = df['class']\n",
    "df = normalized_df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the available dataset to train and test, 80% data of each class to train and 20% to test\n",
    "def split_to_train_test(df, label_column, train_frac=0.8):\n",
    "    train_df, test_df = pd.DataFrame(), pd.DataFrame()\n",
    "    labels = df[label_column].unique()\n",
    "    for lbl in labels:\n",
    "        lbl_df = df[df[label_column] == lbl]\n",
    "        lbl_train_df = lbl_df.sample(frac=train_frac)\n",
    "        lbl_test_df = lbl_df.drop(lbl_train_df.index)\n",
    "        print ('\\nClass %s:\\n---------\\ntotal:%d\\ntrain_df:%d\\ntest_df:%d' % (lbl, len(lbl_df), len(lbl_train_df), len(lbl_test_df)))\n",
    "        train_df = train_df.append(lbl_train_df)\n",
    "        test_df = test_df.append(lbl_test_df)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0:\n",
      "---------\n",
      "total:50\n",
      "train_df:40\n",
      "test_df:10\n",
      "\n",
      "Class 1:\n",
      "---------\n",
      "total:50\n",
      "train_df:40\n",
      "test_df:10\n",
      "\n",
      "Class 2:\n",
      "---------\n",
      "total:50\n",
      "train_df:40\n",
      "test_df:10\n"
     ]
    }
   ],
   "source": [
    "#reset the train dataset and test dataset index\n",
    "train_df, test_df = split_to_train_test(df, 'class', 0.8)\n",
    "train_df = train_df.reset_index().drop('index', axis = 1)\n",
    "test_df = test_df.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38072709e+00,  3.36720285e-01, -1.39346985e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00, -1.24540379e-01, -1.22344235e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.50149039e+00,  1.25924161e+00, -1.56349736e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00,  3.36720285e-01, -1.45014569e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-8.97673879e-01,  7.97980949e-01, -1.28011819e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.25996379e+00,  7.97980949e-01, -1.22344235e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.25996379e+00, -1.24540379e-01, -1.33679402e+00,\n",
       "        -1.17755883e+00],\n",
       "       [-1.74301699e+00, -3.55170711e-01, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-7.76910577e-01,  1.02861128e+00, -1.28011819e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.74301699e+00, -1.24540379e-01, -1.39346985e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.74301699e+00,  3.36720285e-01, -1.39346985e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00,  5.67350617e-01, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.13920048e+00, -1.24540379e-01, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-4.14620671e-01,  1.02861128e+00, -1.39346985e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00,  1.02861128e+00, -1.22344235e+00,\n",
       "        -7.84456844e-01],\n",
       "       [-5.35383973e-01,  1.95113261e+00, -1.39346985e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-7.76910577e-01,  7.97980949e-01, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.13920048e+00,  1.06089953e-01, -1.28011819e+00,\n",
       "        -1.43962681e+00],\n",
       "       [-5.35383973e-01,  7.97980949e-01, -1.28011819e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-1.25996379e+00,  7.97980949e-01, -1.05341485e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.13920048e+00,  1.06089953e-01, -1.28011819e+00,\n",
       "        -1.43962681e+00],\n",
       "       [-1.73094066e-01,  3.10428427e+00, -1.28011819e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-8.97673879e-01,  1.72050228e+00, -1.22344235e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.25996379e+00,  1.06089953e-01, -1.22344235e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.50149039e+00,  1.06089953e-01, -1.28011819e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-8.97673879e-01,  1.72050228e+00, -1.28011819e+00,\n",
       "        -1.17755883e+00],\n",
       "       [-1.01843718e+00,  1.02861128e+00, -1.39346985e+00,\n",
       "        -1.17755883e+00],\n",
       "       [-8.97673879e-01,  1.02861128e+00, -1.33679402e+00,\n",
       "        -1.17755883e+00],\n",
       "       [-7.76910577e-01,  2.41239327e+00, -1.28011819e+00,\n",
       "        -1.43962681e+00],\n",
       "       [-5.23307643e-02,  2.18176294e+00, -1.45014569e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.50149039e+00,  7.97980949e-01, -1.33679402e+00,\n",
       "        -1.17755883e+00],\n",
       "       [-6.56147275e-01,  1.48987194e+00, -1.28011819e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.50149039e+00,  3.36720285e-01, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00,  7.97980949e-01, -1.28011819e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-1.01843718e+00,  7.97980949e-01, -1.22344235e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-8.97673879e-01,  1.48987194e+00, -1.28011819e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-8.97673879e-01,  1.72050228e+00, -1.05341485e+00,\n",
       "        -1.04652483e+00],\n",
       "       [-4.14620671e-01,  2.64302361e+00, -1.33679402e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-5.35383973e-01,  7.97980949e-01, -1.16676652e+00,\n",
       "        -1.30859282e+00],\n",
       "       [-8.97673879e-01,  5.67350617e-01, -1.16676652e+00,\n",
       "        -9.15490838e-01],\n",
       "       [ 1.89195840e-01,  7.97980949e-01,  4.20156854e-01,\n",
       "         5.25883096e-01],\n",
       "       [ 9.13775653e-01, -1.24540379e-01,  3.63481020e-01,\n",
       "         2.63815108e-01],\n",
       "       [ 9.13775653e-01, -3.55170711e-01,  4.76832689e-01,\n",
       "         1.32781114e-01],\n",
       "       [-1.73094066e-01, -1.04706171e+00, -1.46601492e-01,\n",
       "        -2.60320868e-01],\n",
       "       [-1.13920048e+00, -1.50832237e+00, -2.59953162e-01,\n",
       "        -2.60320868e-01],\n",
       "       [-1.01843718e+00, -2.43084370e+00, -1.46601492e-01,\n",
       "        -2.60320868e-01],\n",
       "       [-4.14620671e-01, -1.73895270e+00,  1.36777681e-01,\n",
       "         1.32781114e-01],\n",
       "       [-7.76910577e-01, -8.16431375e-01,  8.01018463e-02,\n",
       "         2.63815108e-01],\n",
       "       [ 4.30722444e-01, -1.96958304e+00,  4.20156854e-01,\n",
       "         3.94849102e-01],\n",
       "       [-1.01843718e+00, -1.73895270e+00, -2.59953162e-01,\n",
       "        -2.60320868e-01],\n",
       "       [-2.93857369e-01, -1.24540379e-01,  1.93453516e-01,\n",
       "         1.32781114e-01],\n",
       "       [-1.73094066e-01, -1.24540379e-01,  2.50129350e-01,\n",
       "         1.74711992e-03],\n",
       "       [-1.73094066e-01, -5.85801043e-01,  4.20156854e-01,\n",
       "         1.32781114e-01],\n",
       "       [ 6.72249049e-01,  3.36720285e-01,  4.20156854e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 1.39682886e+00,  3.36720285e-01,  5.33508524e-01,\n",
       "         2.63815108e-01],\n",
       "       [ 1.03453895e+00,  1.06089953e-01,  3.63481020e-01,\n",
       "         2.63815108e-01],\n",
       "       [ 6.84325379e-02, -1.24540379e-01,  2.50129350e-01,\n",
       "         3.94849102e-01],\n",
       "       [-2.93857369e-01, -8.16431375e-01,  2.50129350e-01,\n",
       "         1.32781114e-01],\n",
       "       [ 3.09959142e-01, -5.85801043e-01,  5.33508524e-01,\n",
       "         1.74711992e-03],\n",
       "       [ 1.15530226e+00, -5.85801043e-01,  5.90184358e-01,\n",
       "         2.63815108e-01],\n",
       "       [-5.23307643e-02, -8.16431375e-01,  8.01018463e-02,\n",
       "         1.74711992e-03],\n",
       "       [-5.23307643e-02, -8.16431375e-01,  1.93453516e-01,\n",
       "        -2.60320868e-01],\n",
       "       [ 1.89195840e-01, -1.96958304e+00,  1.36777681e-01,\n",
       "        -2.60320868e-01],\n",
       "       [-2.93857369e-01, -1.24540379e-01,  4.20156854e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 1.03453895e+00,  1.06089953e-01,  5.33508524e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 6.72249049e-01, -3.55170711e-01,  3.06805185e-01,\n",
       "         1.32781114e-01],\n",
       "       [-5.23307643e-02, -1.04706171e+00,  1.36777681e-01,\n",
       "         1.74711992e-03],\n",
       "       [ 1.89195840e-01, -3.55170711e-01,  4.20156854e-01,\n",
       "         3.94849102e-01],\n",
       "       [-2.93857369e-01, -1.27769204e+00,  8.01018463e-02,\n",
       "        -1.29286874e-01],\n",
       "       [-4.14620671e-01, -1.27769204e+00,  1.36777681e-01,\n",
       "         1.32781114e-01],\n",
       "       [ 3.09959142e-01, -3.55170711e-01,  5.33508524e-01,\n",
       "         2.63815108e-01],\n",
       "       [-1.73094066e-01, -3.55170711e-01,  2.50129350e-01,\n",
       "         1.32781114e-01],\n",
       "       [-4.14620671e-01, -1.50832237e+00,  2.34260117e-02,\n",
       "        -1.29286874e-01],\n",
       "       [-4.14620671e-01, -1.04706171e+00,  3.63481020e-01,\n",
       "         1.74711992e-03],\n",
       "       [ 3.09959142e-01, -1.24540379e-01,  4.76832689e-01,\n",
       "         2.63815108e-01],\n",
       "       [-5.35383973e-01, -1.24540379e-01,  4.20156854e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 1.03453895e+00, -1.24540379e-01,  7.03536028e-01,\n",
       "         6.56917090e-01],\n",
       "       [ 5.51485746e-01, -1.27769204e+00,  6.46860193e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 4.30722444e-01, -3.55170711e-01,  3.06805185e-01,\n",
       "         1.32781114e-01],\n",
       "       [ 1.27606556e+00,  1.06089953e-01,  6.46860193e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 4.30722444e-01, -5.85801043e-01,  5.90184358e-01,\n",
       "         7.87951084e-01],\n",
       "       [ 1.89195840e-01, -1.24540379e-01,  5.90184358e-01,\n",
       "         7.87951084e-01],\n",
       "       [ 2.24217198e+00, -5.85801043e-01,  1.66702522e+00,\n",
       "         1.05001907e+00],\n",
       "       [ 1.27606556e+00,  3.36720285e-01,  1.10026687e+00,\n",
       "         1.44312105e+00],\n",
       "       [ 1.63835547e+00,  1.25924161e+00,  1.32697021e+00,\n",
       "         1.70518904e+00],\n",
       "       [ 1.03453895e+00, -1.27769204e+00,  1.15694270e+00,\n",
       "         7.87951084e-01],\n",
       "       [ 2.24217198e+00, -1.04706171e+00,  1.78037689e+00,\n",
       "         1.44312105e+00],\n",
       "       [ 1.03453895e+00,  5.67350617e-01,  1.10026687e+00,\n",
       "         1.70518904e+00],\n",
       "       [ 5.51485746e-01, -3.55170711e-01,  1.04359104e+00,\n",
       "         7.87951084e-01],\n",
       "       [-5.23307643e-02, -8.16431375e-01,  7.60211862e-01,\n",
       "         9.18985077e-01],\n",
       "       [ 2.24217198e+00, -1.24540379e-01,  1.32697021e+00,\n",
       "         1.44312105e+00],\n",
       "       [ 7.93012351e-01, -1.24540379e-01,  1.15694270e+00,\n",
       "         1.31208706e+00],\n",
       "       [ 7.93012351e-01, -1.24540379e-01,  8.16887697e-01,\n",
       "         1.05001907e+00],\n",
       "       [ 1.03453895e+00, -1.24540379e-01,  8.16887697e-01,\n",
       "         1.44312105e+00],\n",
       "       [ 1.27606556e+00,  1.06089953e-01,  7.60211862e-01,\n",
       "         1.44312105e+00],\n",
       "       [-2.93857369e-01, -5.85801043e-01,  6.46860193e-01,\n",
       "         1.05001907e+00],\n",
       "       [ 7.93012351e-01,  3.36720285e-01,  7.60211862e-01,\n",
       "         1.05001907e+00],\n",
       "       [ 1.87988207e+00, -5.85801043e-01,  1.32697021e+00,\n",
       "         9.18985077e-01],\n",
       "       [ 1.15530226e+00, -1.24540379e-01,  9.86915201e-01,\n",
       "         1.18105307e+00],\n",
       "       [ 1.89195840e-01, -1.96958304e+00,  7.03536028e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 1.63835547e+00,  3.36720285e-01,  1.27029437e+00,\n",
       "         7.87951084e-01],\n",
       "       [ 6.72249049e-01, -5.85801043e-01,  1.04359104e+00,\n",
       "         1.18105307e+00],\n",
       "       [ 6.84325379e-02, -1.24540379e-01,  7.60211862e-01,\n",
       "         7.87951084e-01],\n",
       "       [ 4.30722444e-01,  7.97980949e-01,  9.30239366e-01,\n",
       "         1.44312105e+00],\n",
       "       [ 2.24217198e+00,  1.72050228e+00,  1.66702522e+00,\n",
       "         1.31208706e+00],\n",
       "       [ 1.15530226e+00,  3.36720285e-01,  1.21361854e+00,\n",
       "         1.44312105e+00],\n",
       "       [ 1.63835547e+00, -1.24540379e-01,  1.15694270e+00,\n",
       "         5.25883096e-01],\n",
       "       [ 6.72249049e-01,  1.06089953e-01,  9.86915201e-01,\n",
       "         7.87951084e-01],\n",
       "       [ 5.51485746e-01,  7.97980949e-01,  1.04359104e+00,\n",
       "         1.57415505e+00],\n",
       "       [ 5.51485746e-01, -8.16431375e-01,  6.46860193e-01,\n",
       "         7.87951084e-01],\n",
       "       [ 3.09959142e-01, -1.04706171e+00,  1.04359104e+00,\n",
       "         2.63815108e-01],\n",
       "       [ 2.12140867e+00, -1.24540379e-01,  1.61034938e+00,\n",
       "         1.18105307e+00],\n",
       "       [ 1.75911877e+00, -3.55170711e-01,  1.44032188e+00,\n",
       "         7.87951084e-01],\n",
       "       [ 6.72249049e-01,  3.36720285e-01,  8.73563532e-01,\n",
       "         1.44312105e+00],\n",
       "       [ 1.51759216e+00, -1.24540379e-01,  1.21361854e+00,\n",
       "         1.18105307e+00],\n",
       "       [ 5.51485746e-01, -5.85801043e-01,  7.60211862e-01,\n",
       "         3.94849102e-01],\n",
       "       [ 2.48369858e+00,  1.72050228e+00,  1.49699771e+00,\n",
       "         1.05001907e+00],\n",
       "       [ 5.51485746e-01,  5.67350617e-01,  1.27029437e+00,\n",
       "         1.70518904e+00],\n",
       "       [ 6.72249049e-01, -8.16431375e-01,  8.73563532e-01,\n",
       "         9.18985077e-01],\n",
       "       [-5.23307643e-02, -5.85801043e-01,  7.60211862e-01,\n",
       "         1.57415505e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the train dataset to input and output i.e train_X and train_Y\n",
    "train_X = train_df.iloc[:, 0:4].to_numpy()\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = train_df.iloc[:, 4 : ].to_numpy()\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the test dataset to input and output i.e test_X and test_Y\n",
    "test_X = test_df.iloc[:, 0:4].to_numpy()\n",
    "test_Y = test_df.iloc[:, 4 : ].to_numpy()\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the Neural Network\n",
    "class Neural_Network(object):\n",
    "  def __init__(self):\n",
    "    self.layer_numberofnodes=list()\n",
    "    self.W=list()\n",
    "#     self.inputSize = int(input('No .of nodes in input layer :'))\n",
    "#     self.outputSize = int(input('No. of nodes in output layer :'))\n",
    "#     self.nhiddenlayers= int((input('No of hidden layers :')))\n",
    "    self.inputSize = 4\n",
    "    self.layer_numberofnodes.append(self.inputSize)\n",
    "    self.outputSize = 3\n",
    "    self.nhiddenlayers = 2\n",
    "\n",
    "#     for i in range(self.nhiddenlayers):\n",
    "#       print(\"No. of nodes in hidden layer {} : \".format(i+1))\n",
    "#       self.layer_numberofnodes.append(int(input()))\n",
    "    self.layer_numberofnodes.append(10)\n",
    "    self.layer_numberofnodes.append(6)\n",
    "    self.layer_numberofnodes.append(self.outputSize)\n",
    "\n",
    "    self.layerlist=[None]*(self.nhiddenlayers + 2)\n",
    "\n",
    "    #weights\n",
    "    for i in range(len(self.layer_numberofnodes)-1):\n",
    "      self.W.append(np.random.randn(self.layer_numberofnodes[i],self.layer_numberofnodes[i+1]))\n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "    self.layerlist[0]=X\n",
    "    self.z =  np.dot(X, self.W[0]) \n",
    "    for i in range(1,(len(self.W))):\n",
    "      #print('length layer list iteration {} = {} start'.format(i,len(self.layerlist)))\n",
    "      self.z = self.sigmoid(self.z)\n",
    "      self.layerlist[i]=self.z\n",
    "      self.z = np.dot(self.z,self.W[i])\n",
    "      #print('length layer list iteration {} = {} end'.format(i,len(self.layerlist)))\n",
    "    o=self.sigmoid(self.z)\n",
    "    self.layerlist[self.nhiddenlayers+1]=o\n",
    "    return o \n",
    "\n",
    "  def sigmoid(self, s):\n",
    "    # activation function \n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "  def sigmoidPrime(self, s):\n",
    "    #derivative of sigmoid\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def Relu(self, s):\n",
    "    return np.maximum(0,s)\n",
    "\n",
    "  def ReluPrime(self,s):\n",
    "    if(s==0): \n",
    "      return 0\n",
    "    else:\n",
    "      return 1\n",
    "  def backward(self, X, y, o,lr):\n",
    "\n",
    "    self.z_error = y - o\n",
    "    for i in range(len(self.layerlist)-1,0,-1):\n",
    "      self.z_delta = self.z_error*self.sigmoidPrime(self.layerlist[i])\n",
    "      self.z_error = self.z_delta.dot(self.W[i-1].T ) \n",
    "      self.W[i-1] += lr*self.layerlist[i-1].T.dot(self.z_delta)\n",
    "\n",
    "  def train (self, X, y,lr):\n",
    "    o = self.forward(X)\n",
    "    self.backward(X, y, o,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for different learning rate, check the loss\n",
    "test_loss_values = []\n",
    "learning_rate_values = [float(i) for i in [0.5, 0.1, 0.05, 0.01, 0.001, 0.0001, 0.00001]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  1 \n",
      "Learning rate :  0.5\n",
      "Training Loss: \n",
      "0.9471002186859641\n",
      "Training Loss: \n",
      "0.6664683932707557\n",
      "Training Loss: \n",
      "0.6664017476032881\n",
      "Training Loss: \n",
      "0.6663360459315602\n",
      "Training Loss: \n",
      "0.6662656515840492\n",
      "Training Loss: \n",
      "0.6661848358216557\n",
      "Training Loss: \n",
      "0.6660862392361778\n",
      "Training Loss: \n",
      "0.665958679991667\n",
      "Training Loss: \n",
      "0.6657828816916604\n",
      "Training Loss: \n",
      "0.6655214483684722\n",
      "Test Loss: \n",
      "0.6649864725193706\n",
      "\n",
      "Iteration  2 \n",
      "Learning rate :  0.1\n",
      "Training Loss: \n",
      "1.018054219730134\n",
      "Training Loss: \n",
      "0.6923884662835439\n",
      "Training Loss: \n",
      "0.6448094121222182\n",
      "Training Loss: \n",
      "0.6084192791156486\n",
      "Training Loss: \n",
      "0.5573159012371613\n",
      "Training Loss: \n",
      "0.5030355823758378\n",
      "Training Loss: \n",
      "0.4546591838100214\n",
      "Training Loss: \n",
      "0.4154974232656367\n",
      "Training Loss: \n",
      "0.393795063648498\n",
      "Training Loss: \n",
      "0.37965530401871656\n",
      "Test Loss: \n",
      "0.3736317852272839\n",
      "\n",
      "Iteration  3 \n",
      "Learning rate :  0.05\n",
      "Training Loss: \n",
      "1.0329877979139126\n",
      "Training Loss: \n",
      "0.6941507883299117\n",
      "Training Loss: \n",
      "0.6039999684382477\n",
      "Training Loss: \n",
      "0.5355447015455445\n",
      "Training Loss: \n",
      "0.4710209528088818\n",
      "Training Loss: \n",
      "0.4287439057919798\n",
      "Training Loss: \n",
      "0.4028521556579889\n",
      "Training Loss: \n",
      "0.38665497574512164\n",
      "Training Loss: \n",
      "0.37599863600388334\n",
      "Training Loss: \n",
      "0.36860879792958257\n",
      "Test Loss: \n",
      "0.37340227160071227\n",
      "\n",
      "Iteration  4 \n",
      "Learning rate :  0.01\n",
      "Training Loss: \n",
      "0.840309068249685\n",
      "Training Loss: \n",
      "0.7339825189403959\n",
      "Training Loss: \n",
      "0.6709757376174436\n",
      "Training Loss: \n",
      "0.6371459562700421\n",
      "Training Loss: \n",
      "0.6161867607363939\n",
      "Training Loss: \n",
      "0.6007939240317497\n",
      "Training Loss: \n",
      "0.5880895213044882\n",
      "Training Loss: \n",
      "0.576857444113768\n",
      "Training Loss: \n",
      "0.5665535539525306\n",
      "Training Loss: \n",
      "0.5569262205873536\n",
      "Test Loss: \n",
      "0.56916398699963\n",
      "\n",
      "Iteration  5 \n",
      "Learning rate :  0.001\n",
      "Training Loss: \n",
      "1.068650463443515\n",
      "Training Loss: \n",
      "1.0624767461285882\n",
      "Training Loss: \n",
      "1.0563936777866496\n",
      "Training Loss: \n",
      "1.050404572284194\n",
      "Training Loss: \n",
      "1.0445118754428644\n",
      "Training Loss: \n",
      "1.038717137649832\n",
      "Training Loss: \n",
      "1.0330210074567316\n",
      "Training Loss: \n",
      "1.0274232442544957\n",
      "Training Loss: \n",
      "1.0219227475521488\n",
      "Training Loss: \n",
      "1.0165176000359561\n",
      "Test Loss: \n",
      "1.020788730319582\n",
      "\n",
      "Iteration  6 \n",
      "Learning rate :  0.0001\n",
      "Training Loss: \n",
      "0.7898784810255216\n",
      "Training Loss: \n",
      "0.7894888518979981\n",
      "Training Loss: \n",
      "0.7890992126417116\n",
      "Training Loss: \n",
      "0.7887095671347064\n",
      "Training Loss: \n",
      "0.7883199192553317\n",
      "Training Loss: \n",
      "0.7879302728819947\n",
      "Training Loss: \n",
      "0.7875406318929146\n",
      "Training Loss: \n",
      "0.7871510001658739\n",
      "Training Loss: \n",
      "0.7867613815779722\n",
      "Training Loss: \n",
      "0.7863717800053786\n",
      "Test Loss: \n",
      "0.788854595197851\n",
      "\n",
      "Iteration  7 \n",
      "Learning rate :  1e-05\n",
      "Training Loss: \n",
      "0.6755946337453947\n",
      "Training Loss: \n",
      "0.6755596037539223\n",
      "Training Loss: \n",
      "0.675524582968701\n",
      "Training Loss: \n",
      "0.675489571384953\n",
      "Training Loss: \n",
      "0.6754545689979001\n",
      "Training Loss: \n",
      "0.6754195758027638\n",
      "Training Loss: \n",
      "0.6753845917947645\n",
      "Training Loss: \n",
      "0.6753496169691228\n",
      "Training Loss: \n",
      "0.6753146513210583\n",
      "Training Loss: \n",
      "0.6752796948457906\n",
      "Test Loss: \n",
      "0.6736001065644484\n"
     ]
    }
   ],
   "source": [
    "#for different learning rates, compute the test loss. \n",
    "for i in range(len(learning_rate_values)):\n",
    "    print(\"\\nIteration \", i+1, \"\\nLearning rate : \", learning_rate_values[i])\n",
    "    NN = Neural_Network()\n",
    "    epoch = 10\n",
    "    learning_rate = learning_rate_values[i]\n",
    "    for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "      #print (\"Input: \\n\" + str(X))\n",
    "    #   print (\"Actual Output: \\n\" + str(y))\n",
    "      print (\"Training Loss: \\n\" + str((np.mean(np.square(train_Y - NN.forward(train_X))))))# mean sum squared loss\n",
    "      NN.train(train_X, train_Y, learning_rate)\n",
    "    test_loss_values.append((np.mean(np.square(test_Y - NN.forward(test_X)))))\n",
    "    print (\"Test Loss: \\n\" + str(test_loss_values[i]))# mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'test loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss v/s the learning rate\n",
    "test_loss_values\n",
    "plt.plot(learning_rate_values, test_loss_values, 'ro')\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: \n",
      "0.6134779723465793\n",
      "Training Loss: \n",
      "0.56355775484174\n",
      "Training Loss: \n",
      "0.5058212882600273\n",
      "Training Loss: \n",
      "0.46800839581997095\n",
      "Training Loss: \n",
      "0.43878102879978925\n",
      "Test Loss: \n",
      "0.4293069179420296\n"
     ]
    }
   ],
   "source": [
    "#with the best possible learning rate observed from above, train the neural network and use it to predict\n",
    "NN = Neural_Network()  \n",
    "learning_rate = 0.05\n",
    "NN.train(train_X, train_Y, learning_rate)\n",
    "epoch = 5\n",
    "for q in range(int(epoch)): # trains the NN 1,000 times\n",
    "    print (\"Training Loss: \\n\" + str((np.mean(np.square(train_Y - NN.forward(train_X))))))# mean sum squared loss\n",
    "    NN.train(train_X, train_Y, learning_rate)\n",
    "\n",
    "print (\"Test Loss: \\n\" + str(np.mean(np.square(test_Y - NN.forward(test_X)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions for the test dataset\n",
    "output = NN.forward(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 2, 2, 2, 0, 2, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consider the maximum index in the predictions to get the class number\n",
    "y_predicted = []\n",
    "for row in output : \n",
    "    y_predicted.append(np.argmax(row))\n",
    "y_predicted = np.array(y_predicted)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the actual test outputs by flattening the output test array\n",
    "y_test_output = test_Y.flatten()\n",
    "y_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 2  2  6]\n",
      " [ 0  1  9]]\n"
     ]
    }
   ],
   "source": [
    "#build the confusion matrix for the best learning rate ( considered on basis of loss)\n",
    "confusion_matrix = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "for i in range(30):\n",
    "    confusion_matrix[y_test_output[i]][y_predicted[i]] += 1\n",
    "        \n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  2,  9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate true positive \n",
    "TruePositive = np.diag(confusion_matrix) \n",
    "TruePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 6]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate false positive\n",
    "FalsePositive = []\n",
    "for i in range(3): \n",
    "    FalsePositive.append(sum(confusion_matrix[:,i]) - confusion_matrix[i,i])\n",
    "FalsePositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate false negative\n",
    "FalseNegative = []\n",
    "for i in range(3): \n",
    "    FalseNegative.append(sum(confusion_matrix[i, :]) - confusion_matrix[i,i])\n",
    "FalseNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 19, 14]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate true negative\n",
    "TrueNegative = []\n",
    "for i in range(3): \n",
    "    temp = np.delete(confusion_matrix, i, 0) # delete ith row \n",
    "    temp = np.delete(temp, i, 1) # delete ith column \n",
    "    TrueNegative.append(sum(sum(temp))) \n",
    "TrueNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class  1\n",
      "Accuracy :  0.9333333333333333\n",
      "Precision :  0.8333333333333334\n",
      "Recall :  1.0\n",
      "F-Score :  0.9090909090909091\n",
      "\n",
      "Class  2\n",
      "Accuracy :  0.7\n",
      "Precision :  0.6666666666666666\n",
      "Recall :  0.2\n",
      "F-Score :  0.30769230769230765\n",
      "\n",
      "Class  3\n",
      "Accuracy :  0.7666666666666667\n",
      "Precision :  0.6\n",
      "Recall :  0.9\n",
      "F-Score :  0.7200000000000001\n"
     ]
    }
   ],
   "source": [
    "#calculate classwise precision, accuracy and F-score based on the above data\n",
    "for i in range(3):\n",
    "    print(\"\\nClass \", i+1)\n",
    "    print(\"Accuracy : \", (TruePositive[i] + TrueNegative[i]) / (TruePositive[i] + TrueNegative[i] + FalsePositive[i] + FalseNegative[i]))\n",
    "    precision = (TruePositive[i]) / (TruePositive[i] + FalsePositive[i])\n",
    "    print(\"Precision : \", precision)\n",
    "    recall = (TruePositive[i]) / (TruePositive[i] + FalseNegative[i])\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"F-Score : \", (2 * precision * recall) / (precision + recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
